{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part **A**: Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from gensim.models import keyedvectors\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import jieba\n",
    "import emoji\n",
    "punctuation = str.maketrans({i:'' for i in string.punctuation}) \n",
    "\n",
    "file = \"vector/tencent-ailab-embedding-zh-d200-v0.2.0-s.txt\"\n",
    "WORD2VEC = keyedvectors.load_word2vec_format(file, binary=False)\n",
    "DIM = 200\n",
    "\n",
    "RATIO = [0.7, 0.3]\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH = 64\n",
    "CLASS = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part **B**: Loading data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count=[0, 0, 0, 0]\n",
    "\n",
    "def data_clearning(token):\n",
    "    '''The function performs data cleaning by removing URLs, HTML tags, emojis, and punctuation from a\n",
    "    given token.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    token\n",
    "        The input text that needs to be cleaned of URLs, HTML tags, emojis, and punctuation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        the cleaned version of the input token, with URLs, HTML tags, emojis, and punctuation removed.\n",
    "    \n",
    "    '''\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+') #Remove URL\n",
    "    token = url.sub(r'', token)\n",
    "    html = re.compile(r'<.*?>') #Remove HTML\n",
    "    token = html.sub(r'', token)\n",
    "    token = emoji.demojize(token) #Remove Emoji\n",
    "    space = re.compile(r\"\\s+\")\n",
    "    token = space.sub(r'', token)\n",
    "    token = token.translate(punctuation) #Remove English punctuation\n",
    "    #cn_char = re.compile(r\"[^0-9\\u4e00-\\u9fa5]\")\n",
    "    #token = cn_char.sub(r'', token)\n",
    "    token = token.replace(\"\\ufeff\",\"\") #Remove \\ufeff\n",
    "    return token\n",
    "    \n",
    "def read_csv(filedir=\"data\", mode=\"train\", filename=\"train.csv\"):\n",
    "    \"\"\"Read dataset from csv\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filedir : str, optional\n",
    "        the folder of the dataset, by default \"data\".\n",
    "    mode : bool, optional\n",
    "        whether to load training dataset, by default 'True'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df['text'].to_list(): list()\n",
    "        a list of multiple sentences\n",
    "    related_list: list()\n",
    "        a list of the target for training set, and the id for the testing set\n",
    "\n",
    "    \"\"\"\n",
    "    global class_count\n",
    "    file = os.path.join(filedir, filename)\n",
    "    label_list, token_vector_list = [], []\n",
    "    if (mode == \"train\"):\n",
    "        df = pd.read_csv(file, sep=\",\", encoding=\"utf-8\")\n",
    "        for idx, row_data in df.iterrows():\n",
    "            label = int(row_data[\"label\"])\n",
    "            review = row_data[\"review\"] \n",
    "            token_vector = extract_tokens(review)\n",
    "            if type(token_vector) != np.ndarray: #omit this sample\n",
    "                continue\n",
    "            class_count[label] += 1 #Count number of each class\n",
    "            label_list.append(label)\n",
    "            token_vector_list.append(token_vector)\n",
    "        \n",
    "    else: #Test\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for lines in f:\n",
    "                token_vector = extract_tokens(lines)\n",
    "                if type(token_vector) != np.ndarray: #omit this sample\n",
    "                    continue\n",
    "                token_vector_list.append(token_vector)\n",
    "                label_list.append(lines) #Store Raw message\n",
    "\n",
    "    return token_vector_list, label_list            \n",
    "\n",
    "\n",
    "def extract_tokens(sentence):\n",
    "    sentence = data_clearning(sentence) \n",
    "    vector_list = []\n",
    "    cn_char = re.compile(r\"[0-9\\u4e00-\\u9fa5]\")\n",
    "    for word in jieba.cut(sentence):\n",
    "        if not re.search(cn_char, word):\n",
    "            continue\n",
    "        try:\n",
    "            word_vector = WORD2VEC[word]\n",
    "        except KeyError: #Not in pre-trained context word\n",
    "            continue\n",
    "            #word_vector = np.zeros([DIM], dtype=np.float32)\n",
    "        vector_list.append(np.expand_dims(word_vector, 0))\n",
    "    if len(vector_list):\n",
    "        vectors = np.concatenate(vector_list, 0)\n",
    "    else:\n",
    "        vectors = None #Omit this sample\n",
    "    return vectors\n",
    "\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, filedir=\"data\", mode=\"train\", filename=\"train.csv\"):\n",
    "        super().__init__()\n",
    "        self.targets, self.content = None, None\n",
    "        if (mode==\"train\"): #For training dataset, we focus on texts and targers\n",
    "            self.texts, self.targets = read_csv(filedir, mode, filename)\n",
    "        else: #For testing dataset, we focus on texts and its content\n",
    "            self.texts, self.content  = read_csv(filedir, mode, filename)\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(self.texts[index])\n",
    "        label = torch.tensor(self.targets[index]) if self.mode==\"train\" else None\n",
    "        return x, label\n",
    "    \n",
    "def get_dataloader(file_dir=\"data\"):\n",
    "    '''This function returns three data loaders for training, validation, and testing datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_dir, optional\n",
    "        The directory where the data files are stored.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        The function returns three dataloaders: `train_dataloader`, `val_dataloader`, and`test_dataloader`.\n",
    "    These dataloaders are used to load batches of data during training, validation,and testing of a \n",
    "    machine learning model.\n",
    "    \n",
    "    '''\n",
    "    def collate_fn(batch):\n",
    "        x, y = zip(*batch)\n",
    "        x_pad = pad_sequence(x, batch_first=True) #forces the shape of X's to be matched\n",
    "        if y[0] != None:\n",
    "            y = torch.tensor(y, dtype=int) \n",
    "        return x_pad, y\n",
    "    \n",
    "    train_val_dataset = TweetDataset(file_dir)\n",
    "    train_dataset, val_dataset = random_split(train_val_dataset, RATIO) #Split the dataset with ration 0.7\n",
    "    train_dataloader = DataLoader(train_dataset, \n",
    "                                  batch_size = BATCH,\n",
    "                                  shuffle = True,\n",
    "                                  collate_fn = collate_fn)\n",
    "    val_dataloader = DataLoader(val_dataset, \n",
    "                                batch_size = BATCH,\n",
    "                                shuffle = True,\n",
    "                                collate_fn = collate_fn)\n",
    "    test_dataloader = DataLoader(TweetDataset(mode=\"test\", filename=\"test.txt\"), \n",
    "                                 batch_size = BATCH,\n",
    "                                 shuffle = False,\n",
    "                                 collate_fn = collate_fn)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part **C**: Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_units=128, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "        self.rnn = nn.GRU(DIM, hidden_units, 1, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_units, CLASS)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if name.startswith(\"weight\"):\n",
    "                nn.init.xavier_normal_(param)\n",
    "            else:\n",
    "                nn.init.zeros_(param)\n",
    "        nn.init.orthogonal_(self.linear.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x shape: [batch, max_word_length, embedding_length]\n",
    "        emb = self.drop(x)\n",
    "        output, _ = self.rnn(emb)\n",
    "        output = output[:, -1] #Only cares the output of the last state\n",
    "        output = self.linear(output)\n",
    "        #output = self.softmax(output)\n",
    "        return output\n",
    "    \n",
    "model = RNN().to(DEVICE) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part **D**: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = [], []\n",
    "total_epoch = 50\n",
    "#WEIGHTS = torch.tensor([(1/x) for x in class_count]).to(DEVICE)\n",
    "\n",
    "def train_val():\n",
    "    '''This function contains the training and validation process to train the model.\n",
    "    \n",
    "    '''\n",
    "    global train_loss, val_loss, total_epoch\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=4e-3, weight_decay=1e-4)\n",
    "    citerion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.8, -1)\n",
    "    best_epoch = 0         #Which epoch has the best performance\n",
    "    best_score = 0         #Which epoch has the best performance\n",
    "\n",
    "    for epoch in range(100):\n",
    "        loss_sum = 0\n",
    "        train_len = len(train_dataloader.dataset)\n",
    "        val_len = len(val_dataloader.dataset)\n",
    "        if (epoch - best_epoch > 10):\n",
    "            total_epoch = epoch\n",
    "            print('best epoch %d: %.2f%%'%(best_epoch, best_score*100))\n",
    "            return\n",
    "        \n",
    "        ### Training\n",
    "        model.train()\n",
    "        for x, y in train_dataloader:\n",
    "            batchsize = y.shape[0]\n",
    "            x = x.to(DEVICE) \n",
    "            y = y.to(DEVICE) \n",
    "            hat_y = model(x)\n",
    "            #hat_y = hat_y.squeeze(-1)\n",
    "            loss = citerion(hat_y, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) #Aviod gradient loss\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item() * batchsize\n",
    "        \n",
    "        scheduler.step()\n",
    "        train_loss.append(loss_sum/train_len)\n",
    "        print(f'Epoch {epoch}:\\n[Training] loss: {loss_sum / train_len}')\n",
    "        \n",
    "\n",
    "        ### Validation\n",
    "        accuracy = 0\n",
    "        loss_sum = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad(): #No need to back-propagated during validating\n",
    "            for x, y in val_dataloader:\n",
    "                batchsize = y.shape[0]\n",
    "                x = x.to(DEVICE) \n",
    "                y = y.to(DEVICE) \n",
    "                hat_y = model(x)\n",
    "                #hat_y = hat_y.squeeze(-1)\n",
    "                predictions = hat_y.argmax(dim=1)\n",
    "                score = torch.sum(torch.where(predictions == y, 1, 0)) #Accuracy score of current batch\n",
    "                accuracy += score.item()\n",
    "                loss = citerion(hat_y, y)\n",
    "                loss_sum += loss.item() * batchsize\n",
    "\n",
    "            #Loss    \n",
    "            val_loss.append(loss_sum/val_len)\n",
    "            #Accuracy\n",
    "            accuracy /= len(val_dataloader.dataset)\n",
    "            print('Validation accuracy: %.2f%%.'%(accuracy*100))\n",
    "            if accuracy > best_score: #A better model is found, update and save\n",
    "                best_score = accuracy\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), 'rnn.pth') \n",
    "                print(\"SAVED!!!\")\n",
    "                #print('Best validation accuracy updated to: %.2f%%. Saved.'%(best_score*100))\n",
    "\n",
    "    print('best epoch %d: %.2f%%'%(best_epoch, best_score*100))\n",
    "\n",
    "train_val()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part **E**: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    '''This function contains the training and validation process to train the model.\n",
    "\n",
    "    '''\n",
    "    global df\n",
    "    def save_csv(content_list, target_list,dir=\"Data\"):\n",
    "        global df\n",
    "        dic = {'target':target_list, 'text':content_list}\n",
    "        df = pd.DataFrame(dic)\n",
    "        path = os.path.join(dir,'result.xlsx')\n",
    "        df.to_excel(path,encoding='utf-8')\n",
    "\n",
    "    model.load_state_dict(torch.load('rnn.pth'))\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad(): #No need to back-propagated during validating\n",
    "        for x, _ in test_dataloader: #No y is needed in testing\n",
    "            x = x.to(DEVICE) \n",
    "            hat_y = model(x)\n",
    "            #hat_y = hat_y.squeeze(-1)\n",
    "            hat_y = torch.argmax(hat_y, dim=1) #For probablity>0.5, consider it as the positve label\n",
    "            results.append(hat_y.detach().cpu().numpy())\n",
    "    target_list = np.concatenate(results).tolist()\n",
    "    content_list = test_dataloader.dataset.content #Output the id when reading 'content.csv'\n",
    "    save_csv(content_list, target_list)\n",
    "    \n",
    "test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part **F**: Results demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[95:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDA4220",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
