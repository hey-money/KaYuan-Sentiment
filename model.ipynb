{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part **A**: Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\DDA4220\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from gensim.models import keyedvectors\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import jieba\n",
    "import emoji\n",
    "punctuation = str.maketrans({i:'' for i in string.punctuation}) \n",
    "\n",
    "file = \"vector/tencent-ailab-embedding-zh-d200-v0.2.0-s.txt\"\n",
    "WORD2VEC = keyedvectors.load_word2vec_format(file, binary=False)\n",
    "DIM = 200\n",
    "\n",
    "RATIO = [0.7, 0.3]\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH = 64\n",
    "CLASS = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part **B**: Loading data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count=[0, 0, 0, 0]\n",
    "\n",
    "def data_clearning(token):\n",
    "    '''The function performs data cleaning by removing URLs, HTML tags, emojis, and punctuation from a\n",
    "    given token.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    token\n",
    "        The input text that needs to be cleaned of URLs, HTML tags, emojis, and punctuation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        the cleaned version of the input token, with URLs, HTML tags, emojis, and punctuation removed.\n",
    "    \n",
    "    '''\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+') #Remove URL\n",
    "    token = url.sub(r'', token)\n",
    "    html = re.compile(r'<.*?>') #Remove HTML\n",
    "    token = html.sub(r'', token)\n",
    "    token = emoji.demojize(token) #Remove Emoji\n",
    "    space = re.compile(r\"\\s+\")\n",
    "    token = space.sub(r'', token)\n",
    "    token = token.translate(punctuation) #Remove English punctuation\n",
    "    #cn_char = re.compile(r\"[^0-9\\u4e00-\\u9fa5]\")\n",
    "    #token = cn_char.sub(r'', token)\n",
    "    token = token.replace(\"\\ufeff\",\"\") #Remove \\ufeff\n",
    "    return token\n",
    "    \n",
    "def read_csv(filedir=\"data\", mode=\"train\", filename=\"train.csv\"):\n",
    "    \"\"\"Read dataset from csv\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filedir : str, optional\n",
    "        the folder of the dataset, by default \"data\".\n",
    "    mode : bool, optional\n",
    "        whether to load training dataset, by default 'True'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df['text'].to_list(): list()\n",
    "        a list of multiple sentences\n",
    "    related_list: list()\n",
    "        a list of the target for training set, and the id for the testing set\n",
    "\n",
    "    \"\"\"\n",
    "    global class_count\n",
    "    file = os.path.join(filedir, filename)\n",
    "    label_list, token_vector_list = [], []\n",
    "    if (mode == \"train\"):\n",
    "        df = pd.read_csv(file, sep=\",\", encoding=\"utf-8\")\n",
    "        for idx, row_data in df.iterrows():\n",
    "            label = int(row_data[\"label\"])\n",
    "            review = row_data[\"review\"] \n",
    "            token_vector = extract_tokens(review)\n",
    "            if type(token_vector) != np.ndarray: #omit this sample\n",
    "                continue\n",
    "            class_count[label] += 1 #Count number of each class\n",
    "            label_list.append(label)\n",
    "            token_vector_list.append(token_vector)\n",
    "        \n",
    "    else: #Test\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for lines in f:\n",
    "                token_vector = extract_tokens(lines)\n",
    "                if type(token_vector) != np.ndarray: #omit this sample\n",
    "                    continue\n",
    "                token_vector_list.append(token_vector)\n",
    "                label_list.append(lines) #Store Raw message\n",
    "\n",
    "    return token_vector_list, label_list            \n",
    "\n",
    "\n",
    "def extract_tokens(sentence):\n",
    "    sentence = data_clearning(sentence) \n",
    "    vector_list = []\n",
    "    cn_char = re.compile(r\"[0-9\\u4e00-\\u9fa5]\")\n",
    "    for word in jieba.cut(sentence):\n",
    "        if not re.search(cn_char, word):\n",
    "            continue\n",
    "        try:\n",
    "            word_vector = WORD2VEC[word]\n",
    "        except KeyError: #Not in pre-trained context word\n",
    "            continue\n",
    "            #word_vector = np.zeros([DIM], dtype=np.float32)\n",
    "        vector_list.append(np.expand_dims(word_vector, 0))\n",
    "    if len(vector_list):\n",
    "        vectors = np.concatenate(vector_list, 0)\n",
    "    else:\n",
    "        vectors = None #Omit this sample\n",
    "    return vectors\n",
    "\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, filedir=\"data\", mode=\"train\", filename=\"train.csv\"):\n",
    "        super().__init__()\n",
    "        self.targets, self.content = None, None\n",
    "        if (mode==\"train\"): #For training dataset, we focus on texts and targers\n",
    "            self.texts, self.targets = read_csv(filedir, mode, filename)\n",
    "        else: #For testing dataset, we focus on texts and its content\n",
    "            self.texts, self.content  = read_csv(filedir, mode, filename)\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(self.texts[index])\n",
    "        label = torch.tensor(self.targets[index]) if self.mode==\"train\" else None\n",
    "        return x, label\n",
    "    \n",
    "def get_dataloader(file_dir=\"data\"):\n",
    "    '''This function returns three data loaders for training, validation, and testing datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_dir, optional\n",
    "        The directory where the data files are stored.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        The function returns three dataloaders: `train_dataloader`, `val_dataloader`, and`test_dataloader`.\n",
    "    These dataloaders are used to load batches of data during training, validation,and testing of a \n",
    "    machine learning model.\n",
    "    \n",
    "    '''\n",
    "    def collate_fn(batch):\n",
    "        x, y = zip(*batch)\n",
    "        x_pad = pad_sequence(x, batch_first=True) #forces the shape of X's to be matched\n",
    "        if y[0] != None:\n",
    "            y = torch.tensor(y, dtype=int) \n",
    "        return x_pad, y\n",
    "    \n",
    "    train_val_dataset = TweetDataset(file_dir)\n",
    "    train_dataset, val_dataset = random_split(train_val_dataset, RATIO) #Split the dataset with ration 0.7\n",
    "    train_dataloader = DataLoader(train_dataset, \n",
    "                                  batch_size = BATCH,\n",
    "                                  shuffle = True,\n",
    "                                  collate_fn = collate_fn)\n",
    "    val_dataloader = DataLoader(val_dataset, \n",
    "                                batch_size = BATCH,\n",
    "                                shuffle = True,\n",
    "                                collate_fn = collate_fn)\n",
    "    test_dataloader = DataLoader(TweetDataset(mode=\"test\", filename=\"test.txt\"), \n",
    "                                 batch_size = BATCH,\n",
    "                                 shuffle = False,\n",
    "                                 collate_fn = collate_fn)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = get_dataloader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part **C**: Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_units=128, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "        self.rnn = nn.GRU(DIM, hidden_units, 1, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_units, CLASS)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if name.startswith(\"weight\"):\n",
    "                nn.init.xavier_normal_(param)\n",
    "            else:\n",
    "                nn.init.zeros_(param)\n",
    "        nn.init.orthogonal_(self.linear.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x shape: [batch, max_word_length, embedding_length]\n",
    "        emb = self.drop(x)\n",
    "        output, _ = self.rnn(emb)\n",
    "        output = output[:, -1] #Only cares the output of the last state\n",
    "        output = self.linear(output)\n",
    "        #output = self.softmax(output)\n",
    "        return output\n",
    "    \n",
    "model = RNN().to(DEVICE) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part **D**: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "[Training] loss: 1.1776251361353685\n",
      "Validation accuracy: 56.52%.\n",
      "SAVED!!!\n",
      "Epoch 1:\n",
      "[Training] loss: 1.0830041780433404\n",
      "Validation accuracy: 58.39%.\n",
      "SAVED!!!\n",
      "Epoch 2:\n",
      "[Training] loss: 1.0547484328138108\n",
      "Validation accuracy: 58.99%.\n",
      "SAVED!!!\n",
      "Epoch 3:\n",
      "[Training] loss: 1.0476120120732328\n",
      "Validation accuracy: 59.30%.\n",
      "SAVED!!!\n",
      "Epoch 4:\n",
      "[Training] loss: 1.0433031251902694\n",
      "Validation accuracy: 59.43%.\n",
      "SAVED!!!\n",
      "Epoch 5:\n",
      "[Training] loss: 1.039715209911414\n",
      "Validation accuracy: 59.36%.\n",
      "Epoch 6:\n",
      "[Training] loss: 1.037203197174186\n",
      "Validation accuracy: 59.64%.\n",
      "SAVED!!!\n",
      "Epoch 7:\n",
      "[Training] loss: 1.0348975002298917\n",
      "Validation accuracy: 59.77%.\n",
      "SAVED!!!\n",
      "Epoch 8:\n",
      "[Training] loss: 1.0320522794540443\n",
      "Validation accuracy: 59.88%.\n",
      "SAVED!!!\n",
      "Epoch 9:\n",
      "[Training] loss: 1.0302372755814009\n",
      "Validation accuracy: 59.80%.\n",
      "Epoch 10:\n",
      "[Training] loss: 1.0284420857529881\n",
      "Validation accuracy: 59.83%.\n",
      "Epoch 11:\n",
      "[Training] loss: 1.0270170751977155\n",
      "Validation accuracy: 59.92%.\n",
      "SAVED!!!\n",
      "Epoch 12:\n",
      "[Training] loss: 1.026739576810497\n",
      "Validation accuracy: 59.85%.\n",
      "Epoch 13:\n",
      "[Training] loss: 1.025497101208337\n",
      "Validation accuracy: 59.85%.\n",
      "Epoch 14:\n",
      "[Training] loss: 1.0244987735440507\n",
      "Validation accuracy: 60.06%.\n",
      "SAVED!!!\n",
      "Epoch 15:\n",
      "[Training] loss: 1.0239311418377468\n",
      "Validation accuracy: 59.95%.\n",
      "Epoch 16:\n",
      "[Training] loss: 1.022624646977133\n",
      "Validation accuracy: 60.03%.\n",
      "Epoch 17:\n",
      "[Training] loss: 1.023083904867517\n",
      "Validation accuracy: 60.10%.\n",
      "SAVED!!!\n",
      "Epoch 18:\n",
      "[Training] loss: 1.0219348878479977\n",
      "Validation accuracy: 60.02%.\n",
      "Epoch 19:\n",
      "[Training] loss: 1.0213934029229141\n",
      "Validation accuracy: 60.04%.\n",
      "Epoch 20:\n",
      "[Training] loss: 1.0213256153576467\n",
      "Validation accuracy: 59.99%.\n",
      "Epoch 21:\n",
      "[Training] loss: 1.0217902112830264\n",
      "Validation accuracy: 59.97%.\n",
      "Epoch 22:\n",
      "[Training] loss: 1.0204060042973604\n",
      "Validation accuracy: 60.01%.\n",
      "Epoch 23:\n",
      "[Training] loss: 1.0207128012331121\n",
      "Validation accuracy: 60.03%.\n",
      "Epoch 24:\n",
      "[Training] loss: 1.020349889508977\n",
      "Validation accuracy: 60.06%.\n",
      "Epoch 25:\n",
      "[Training] loss: 1.0197764014001995\n",
      "Validation accuracy: 59.99%.\n",
      "Epoch 26:\n",
      "[Training] loss: 1.019221307082987\n",
      "Validation accuracy: 60.02%.\n",
      "Epoch 27:\n",
      "[Training] loss: 1.0199945364447993\n",
      "Validation accuracy: 60.06%.\n",
      "best epoch 17: 60.10%\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = [], []\n",
    "total_epoch = 50\n",
    "#WEIGHTS = torch.tensor([(1/x) for x in class_count]).to(DEVICE)\n",
    "\n",
    "def train_val():\n",
    "    '''This function contains the training and validation process to train the model.\n",
    "    \n",
    "    '''\n",
    "    global train_loss, val_loss, total_epoch\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=4e-3, weight_decay=1e-4)\n",
    "    citerion = torch.nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.8, -1)\n",
    "    best_epoch = 0         #Which epoch has the best performance\n",
    "    best_score = 0         #Which epoch has the best performance\n",
    "\n",
    "    for epoch in range(100):\n",
    "        loss_sum = 0\n",
    "        train_len = len(train_dataloader.dataset)\n",
    "        val_len = len(val_dataloader.dataset)\n",
    "        if (epoch - best_epoch > 10):\n",
    "            total_epoch = epoch\n",
    "            print('best epoch %d: %.2f%%'%(best_epoch, best_score*100))\n",
    "            return\n",
    "        \n",
    "        ### Training\n",
    "        model.train()\n",
    "        for x, y in train_dataloader:\n",
    "            batchsize = y.shape[0]\n",
    "            x = x.to(DEVICE) \n",
    "            y = y.to(DEVICE) \n",
    "            hat_y = model(x)\n",
    "            #hat_y = hat_y.squeeze(-1)\n",
    "            loss = citerion(hat_y, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) #Aviod gradient loss\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item() * batchsize\n",
    "        \n",
    "        scheduler.step()\n",
    "        train_loss.append(loss_sum/train_len)\n",
    "        print(f'Epoch {epoch}:\\n[Training] loss: {loss_sum / train_len}')\n",
    "        \n",
    "\n",
    "        ### Validation\n",
    "        accuracy = 0\n",
    "        loss_sum = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad(): #No need to back-propagated during validating\n",
    "            for x, y in val_dataloader:\n",
    "                batchsize = y.shape[0]\n",
    "                x = x.to(DEVICE) \n",
    "                y = y.to(DEVICE) \n",
    "                hat_y = model(x)\n",
    "                #hat_y = hat_y.squeeze(-1)\n",
    "                predictions = hat_y.argmax(dim=1)\n",
    "                score = torch.sum(torch.where(predictions == y, 1, 0)) #Accuracy score of current batch\n",
    "                accuracy += score.item()\n",
    "                loss = citerion(hat_y, y)\n",
    "                loss_sum += loss.item() * batchsize\n",
    "\n",
    "            #Loss    \n",
    "            val_loss.append(loss_sum/val_len)\n",
    "            #Accuracy\n",
    "            accuracy /= len(val_dataloader.dataset)\n",
    "            print('Validation accuracy: %.2f%%.'%(accuracy*100))\n",
    "            if accuracy > best_score: #A better model is found, update and save\n",
    "                best_score = accuracy\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), 'rnn.pth') \n",
    "                print(\"SAVED!!!\")\n",
    "                #print('Best validation accuracy updated to: %.2f%%. Saved.'%(best_score*100))\n",
    "\n",
    "    print('best epoch %d: %.2f%%'%(best_epoch, best_score*100))\n",
    "\n",
    "\n",
    "    \n",
    "#if __name__ == \"__main__\":\n",
    "#    train_val()\n",
    "#    #test()\n",
    "#    print(\"Loss %.4f\"%val_loss[-1])\n",
    "train_val()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part **E**: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\DDA4220\\lib\\site-packages\\pandas\\util\\_decorators.py:211: FutureWarning: the 'encoding' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'encoding'\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    '''This function contains the training and validation process to train the model.\n",
    "\n",
    "    '''\n",
    "    global df\n",
    "    def save_csv(content_list, target_list,dir=\"Data\"):\n",
    "        global df\n",
    "        dic = {'target':target_list, 'text':content_list}\n",
    "        df = pd.DataFrame(dic)\n",
    "        path = os.path.join(dir,'result.xlsx')\n",
    "        df.to_excel(path,encoding='utf-8')\n",
    "\n",
    "    model.load_state_dict(torch.load('rnn.pth'))\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad(): #No need to back-propagated during validating\n",
    "        for x, _ in test_dataloader: #No y is needed in testing\n",
    "            x = x.to(DEVICE) \n",
    "            hat_y = model(x)\n",
    "            #hat_y = hat_y.squeeze(-1)\n",
    "            hat_y = torch.argmax(hat_y, dim=1) #For probablity>0.5, consider it as the positve label\n",
    "            results.append(hat_y.detach().cpu().numpy())\n",
    "    target_list = np.concatenate(results).tolist()\n",
    "    content_list = test_dataloader.dataset.content #Output the id when reading 'content.csv'\n",
    "    save_csv(content_list, target_list)\n",
    "    \n",
    "test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part **F**: Results demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>1学分，小问题\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>纯抄的才会0分吧？\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2</td>\n",
       "      <td>我这次也是0前两次满分，吐血了\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>1. 有没有把代码发给别人过  2. 有没有抄别人的代码\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>好难吃 思廷的水果捞[愤怒][愤怒]\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>感觉西瓜很好吃，切的是条状[尖叫]\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0</td>\n",
       "      <td>蹲蹲大三报了校外导师项目但不打算去5.20见面晚宴的uu[尖叫]\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>收晚宴门票价格你开\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0</td>\n",
       "      <td>想问一下gfn各个老师评价！感谢 暑课想上\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0</td>\n",
       "      <td>直接冲zhanghonghui 人美心善的可爱大姐姐 超级随和\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                text\n",
       "95        0                           1学分，小问题\\n\n",
       "96        0                         纯抄的才会0分吧？\\n\n",
       "97        2                   我这次也是0前两次满分，吐血了\\n\n",
       "98        0      1. 有没有把代码发给别人过  2. 有没有抄别人的代码\\n\n",
       "99        1                好难吃 思廷的水果捞[愤怒][愤怒]\\n\n",
       "100       0                 感觉西瓜很好吃，切的是条状[尖叫]\\n\n",
       "101       0  蹲蹲大三报了校外导师项目但不打算去5.20见面晚宴的uu[尖叫]\\n\n",
       "102       0                         收晚宴门票价格你开\\n\n",
       "103       0             想问一下gfn各个老师评价！感谢 暑课想上\\n\n",
       "104       0   直接冲zhanghonghui 人美心善的可爱大姐姐 超级随和\\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[95:105]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDA4220",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
